\documentclass[11pt]{article}

%  USE PACKAGES  ---------------------- 
\usepackage[margin=0.75in,vmargin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref,color}
\usepackage{enumitem,amssymb}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\newlist{todolist}{itemize}{4}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\usepackage{xcolor}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\HREF}[2]{\href{#1}{#2}}
%  -------------------------------------------- 

%  HEADER AND FOOTER (DO NOT EDIT) ----------------------
\newcommand{\problemnumber}{0}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{IE332 Project 2 Report}}
\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
\fancyfoot[L]{IE 332}
\fancyfoot[C]{Project Report 2}
\fancyfoot[R]{Page \thepage}
\renewcommand{\footrulewidth}{0.4pt}

%  --------------------------------------------


%  COVER SHEET (WRITE YOUR FULL NAME(S) WHERE INDICATED!) ----------------------
\newcommand{\addcoversheet}{
\clearpage
\thispagestyle{empty}
\vspace*{0.5in}

\begin{center}
\Huge{{\bf IE332 Group 5 Project Report 2}} % <-- replace with correct assignment #

Due: April 28th, 11:59pm EST % <-- replace with correct due date and time
\end{center}

\vspace{0.2in}

\noindent {\em ``As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do.
Accountable together - we are Purdue.''}

\vspace{0.3in}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|ccccc|c|c}
      Student & Alg. Dev. & Complex. Analysis & Implement. &  Perf. Analysis/Tests & Report & Ovrll & DIFF\\
      \hline
      Henry Liu & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Keegan Kell & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Francis Stolorz & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Jair Zenil M. & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      \hline
      St Dev & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{tabular}
  \end{center}
\end{table}

\vspace{0.2in}

\noindent Date: \today.
}

\begin{document}

\addcoversheet
\setlength{\headheight}{14pt}
% BEGIN YOUR ASSIGNMENT HERE:
\newpage
\tableofcontents

\newpage
\section{Description}
This project involves training a voting-based optimization algorithm to perform adversarial attacks on a binary image classifier. The goal is to trick the classifier by changing a specific number of pixels in the image. The classifier, which has 100\% accuracy on a set of training data, accepts an image and attempts to identify it as either X or Y based on the preponderance of the evidence. An optimizer must provide weights to the algorithms before selecting them based on their projected performance in order to attack the classifier. The majority votes of the algorithms determine the compound classifier's output.\\

Five distinct machine learning and/or optimization techniques must be employed independently to attempt to trick the classifier in order to accomplish this project successfully. The algorithms will be placed inside of another algorithm, which must provide them weights based on how well they should perform given the image. The optimizer must then decide which pixels to use out of the set that the algorithms have chosen to alter. A picture and a scalar (that informs the algorithm of the expected budget in terms of the ratio of the image size) will both be inputs to the adversarial function. The group as a whole will pick five sub-algorithms, each one being assigned to each individual group member.\\

The number of successfully deceived photographs at a specific budget level will be used to evaluate the performance of the algorithms. The smaller the budget, the more valuable each successfully fooled image is, and this scales with the size of the image. The project's success criteria are that the model must successfully fool the classifier with exactly B (the pixel budget) pixels changed in the image, and this must be done in less than 10 seconds per image. The model must be able to fool the classifier with a budget of P/100 for at least one image, where P is the number of pixels in the image.\\

To complete this project, a complete justification of the rationale for selecting each of the five machine learning algorithms will be provided. This justification will also cover how they were trained and how the optimizer allocates the votes. Finally, the justification should explain why such an approach should work. All code, notes, documentation, etc. must be tracked using Git and Github for verification of the effort contributed by each team member. 

\section{Problem Being Addressed}
The main problem that our team is aiming to address in this project is to train an algorithm that will perform adversarial attacks on a binary image classifier. In order to accomplish this, the team worked on developing five sub-algorithms to independently attempt to deceive the given classifier. Our team decided that we would independently work on completing the sub-algorithms and then shift towards developing the weighted majority classifier. We believe that by dividing the labor among the group, each team member would be able to specialize in an algorithm and provide higher quality results. Once weights have been assigned, the highest scoring algorithm would be chosen for use against the model. For a successful algorithm, it is expected that it all images in each respective folder, dandelion and grass, would be returned as the model is structured so that if it is unable to categorize the image correctly with a probability greater than fifty percent. 


\newpage
\section{Solution to the Problem}

\newpage
\noindent\section{Selection of Algorithms}
As a group, we have selected five sub-algorithms that we believe will work best for our overall algorithm. These sub-algorithms were carefully selected, and each one was assigned to a group member. Below is a description of each algorithm and how it works. 
\subsection{Fast Gradient Sign Method}
The Fast Gradient Sign Method is a technique used in adversarial machine learning to generate adversarial examples that can fool a machine learning model. The FGSM algorithm takes advantage of the gradient information of the loss function with respect to the input data to generate adversarial examples.\\

To generate an adversarial example using FGSM, the algorithm first computes the gradient of the loss function with respect to the input data. Then, it uses the sign of the gradient to generate a perturbation vector that is added to the original input to create the adversarial example. The magnitude of the perturbation vector is controlled by a hyperparameter epsilon, which determines how far the perturbation can deviate from the original input.\\

The resulting adversarial example is designed to be as close to the original input as possible while still causing the model to make a misclassification. The goal of this attack is to expose the model's weaknesses and vulnerabilities, and to evaluate its robustness and security against potential real-world attacks. The FGSM algorithm is simple and efficient, and can be applied to various types of machine learning models, including neural networks, decision trees, and support vector machines.\\

We chose the FGSM method because it is fast, computationally efficient, and easy to implement, making it a suitable choice for adversarial attacks. It has been shown to be effective against a wide range of deep learning models, including image classifiers. FGSM can successfully fool binary image classifiers with a relatively small number of perturbations, even with a low pixel budget. Additionally, FGSM can be used to generate a large number of adversarial examples quickly, which is important when optimizing the weights of the different sub-algorithms.
\subsection{Basic Iterative Method}
The Basic Iterative Method is a technique that is an extension of the FGSM. Instead of taking large steps, the BIM uses smaller steps and iterations to change the image, which results in higher rate of success. BIM is very straightforward and easy to manipulate using the right libraries in r, we can easily change the input values so that the algorithm run faster for perform better. \\


\subsection{Deep Fool} % WHAT you chose, WHY, EXPECTATIONS for runtime/performance tradeoffs, 


\subsection{Carlini-Wagner}

The CW attack algorithm is an optimization-based adversarial attack that generates perturbations in the input data to deceive deep learning models. The algorithm focuses on crafting adversarial examples with minimal perturbation to the original input, making it challenging for humans to perceive the difference between the original and the modified input. The CW attack algorithm is highly effective in fooling various deep learning models, including image classifiers, even when the models implement robust defenses.\\

In the given problem, we have a pre-trained image classification model that classifies images as either X or Y, based on the preponderance of evidence. The classifier considers a 51 percent probability of an image being an X as sufficient to classify it as an X. Our goal is to use the CW attack algorithm to fool the classifier into misclassifying the images while adhering to a specific pixel budget. The pixel budget determines the number of pixels that can be altered in the image, which is expressed as a ratio of the total image size (e.g., 0.01 uses 1 percent of the available pixels, 1/P uses only one pixel).\\

The CW attack algorithm operates by solving an optimization problem that seeks to minimize the perturbation added to the original input while maximizing the classifier's misclassification probability. The algorithm comprises three main components, the objective function, binary search, and the gradient descent. The objective function consists of two terms. The first term measures the difference between the original input and the adversarial example, while the second term represents the misclassification probability. The algorithm aims to minimize the objective function by finding the optimal balance between minimizing perturbation and maximizing misclassification. The CW attack employs a binary search strategy to find the optimal balance between the two terms in the objective function. By adjusting a hyperparameter called the "confidence," the algorithm progressively searches for an adversarial example that can successfully fool the classifier. Finally, the algorithm utilizes gradient descent to iteratively update the adversarial example. It calculates the gradient of the objective function with respect to the input and uses it to adjust the adversarial example in the direction that minimizes the objective function.

\subsubsection{Generating Adverserial Image}

The function "generateadversarialexample" generates adversarial images. The function takes three arguments: the input image, the pixelbudgetratio, and the pre-trained model. The goal of this function is to create an adversarial image that fools the classifier while altering a limited number of pixels according to the specified pixel budget ratio.\\

The entire code can be found in Appendix 8.1.1. In the first line the input image is normalized by dividing its pixel value by 255.Normalizing the image ensures that the pixel values fall within the range [0, 1], which is the standard input format for most deep learning models. Next, the pixel budget is calculated by multiplying the total number of pixels in the image (obtained using prod(dim(image))) with the pixelbudgetratio. The ceiling() function rounds up the result to the nearest integer value, ensuring that the pixel budget is a whole number.\\

The Carlini-Wagner attack using the CarliniWagnerL2-function from the 'cleverhans' library. The function takes several arguments, such as the TensorFlow session (sess), the pre-trained model (model), the number of classes (nbclasses), and various hyperparameters controlling the attack's behavior. The maxperturbations parameter is set to the calculated pixelbudget to limit the number of pixels that can be altered in the image. The adverserial image is generated by calling the attack object as generated. 

\subsubsection{Testing of the Carlini-Wagner algorithim}

 The goal of the script is to see if the generated adverserial image actually fooled the classifier. The entire code can be found in the Appendix section 8.1.2\\

The par(mfrow = c(1, 2)) function call sets up a plotting area with one row and two columns, allowing us to display two images side by side. The two plot functions display the original and adversarial images, respectively, with their corresponding titles. This visual comparison helps us confirm that the adversarial image is very similar to the original image, with only subtle differences introduced by the Carlini-Wagner attack algorithm.\\

To test the classifier, the predict() function is used to obtain the model's output probabilities for each class (X or Y) when presented with the original and adversarial images. The array reshape() function reshapes the images into a format suitable for the classifier. The which.max() function returns the index of the maximum probability, which corresponds to the predicted class.\\

Finally, the cat() function to display the predicted classes for the original and adversarial images. If the Carlini-Wagner attack algorithm is successful, the classifier's prediction for the adversarial image should differ from the original image's prediction, despite the visual similarity between the two images. This outcome demonstrates the algorithm's effectiveness in fooling the image classifier while adhering to the specified pixel budget.\\

\subsection{Projected Gradient Descent}

\newpage
\section{Correctness of Proofs}

\newpage
\section{Complexity Analysis}

\clearpage
\section{Appendix}

\subsection{Carlini-Wagner function}
\subsubsection{Adverserial Image Generation}

Below is the function that generates adverserial images using the Carlini Wagner method

\begin{lstlisting}[language=R, frame=single]
generateadversarialexample <- function(image, pixelbudgetratio, model) {
  # Normalize the input image
  image <- image / 255
  
  # Calculate the pixel budget
  pixelbudget <- ceiling(prod(dim(image)) * pixelbudgetratio)
  
  # Initialize the Carlini-Wagner attack
  attack <- CarliniWagnerL2(sess, model, nb_classes = 2, batch_size = 1,
                             confidence = 0, targeted = FALSE, learning_rate = 0.01,
                             binary_search_steps = 9, max_iterations = 1000,
                             abort_early = TRUE, initial_const = 0.01,
                             clip_min = 0, clip_max = 1,
                             maxperturbations = pixelbudget)
  
  # Generate the adversarial example
  adversarial_image <- attack$attack(image)
  
  return(adversarial_image)
}

# generate adverserial image

# Load an image from your dataset
image <- ... # Load your specific image here

# Set the pixel budget ratio
pixel_budget_ratio <- 0.01  # Adjust this value as needed

# Generate the adversarial example
adversarial_image <- generate_adversarial_example(image, pixel_budget_ratio, model)
\end{lstlisting}

\subsubsection{Testing}

\begin{lstlisting}[language=R, frame=single]

# Display the original and adversarial images
par(mfrow = c(1, 2))
plot(as.raster(image), main = "Original Image")
plot(as.raster(adversarial_image), main = "Adversarial Image")

# Test the classifier
original_pred <- which.max(predict(model, array_reshape(image, c(1, dim(image)))))
adversarial_pred <- which.max(predict(model, array_reshape(adversarial_image, c(1, dim(adversarial_image)))))

cat("Original Image Prediction:", original_pred, "\n")
cat("Adversarial Image Prediction:", adversarial_pred, "\n")

\end{lstlisting}

\subsection{Testing/Correctness/Verification}

\newpage\subsection{Runtime Complexity and Walltime}

\newpage\subsection{Performance}

\newpage\subsection{Final Algorithm Selection Justification}

The final algorithm will run through all 5 sub-algorithms (Fast Gradient Sign, Basic Iterative, Deep Fool, Carlini-Wagner, and Projected Gradient) at 10 iterations. For each algorithm a "success index" is made based on the number of successfully fooled images at 10 iterations per image. The algorithm with the highest success index is the one that will be chosen to run through all the iterations.  

\newpage
\section{References}
\begin{enumerate}
    \item ewatson2. (n.d.). Ewatson2/EEL6812\textunderscore DeepFool\textunderscore Project: Class project implementing the deepfool adversarial attack. GitHub. Retrieved April 28, 2023, from https://github.com/ewatson2/EEL6812\textunderscore DeepFool\textunderscore Project 
    
    \item Morgan, A. (2022, May 2). A review of DeepFool: A simple and accurate method to fool Deep Neural Networks. Medium. Retrieved April 28, 2023, from https://medium.com/machine-intelligence-and-deep-learning-lab/a-review-of-deepfool-a-simple-and-accurate-method-to-fool-deep-neural-networks-b016fba9e48e 
    
\end{enumerate}

\end{document}
