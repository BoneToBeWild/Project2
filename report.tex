\documentclass[11pt]{article}

%  USE PACKAGES  ---------------------- 
\usepackage[margin=0.75in,vmargin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref,color}
\usepackage{enumitem,amssymb}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\newlist{todolist}{itemize}{4}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\usepackage{xcolor}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\HREF}[2]{\href{#1}{#2}}
%  -------------------------------------------- 

%  HEADER AND FOOTER (DO NOT EDIT) ----------------------
\newcommand{\problemnumber}{0}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{IE332 Project 2 Report}}
\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
\fancyfoot[L]{IE 332}
\fancyfoot[C]{Project Report 2}
\fancyfoot[R]{Page \thepage}
\renewcommand{\footrulewidth}{0.4pt}

%  --------------------------------------------


%  COVER SHEET (WRITE YOUR FULL NAME(S) WHERE INDICATED!) ----------------------
\newcommand{\addcoversheet}{
\clearpage
\thispagestyle{empty}
\vspace*{0.5in}

\begin{center}
\Huge{{\bf IE332 Group 5 Project Report 2}} % <-- replace with correct assignment #

Due: April 28th, 11:59pm EST % <-- replace with correct due date and time
\end{center}

\vspace{0.2in}

\noindent {\em ``As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do.
Accountable together - we are Purdue.''}

\vspace{0.3in}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|ccccc|c|c}
      Student & Alg. Dev. & Complex. Analysis & Implement. &  Perf. Analysis/Tests & Report & Ovrll & DIFF\\
      \hline
      Henry Liu & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Keegan Kell & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Francis Stolorz & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Jair Zenil M. & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      \hline
      St Dev & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{tabular}
  \end{center}
\end{table}

\vspace{0.2in}

\noindent Date: \today.
}

\begin{document}
\sloppy

\addcoversheet
\setlength{\headheight}{14pt}
% BEGIN YOUR ASSIGNMENT HERE:
\newpage
\tableofcontents

\newpage
\section{Description}
This project involves training a voting-based optimization algorithm to perform adversarial attacks on a binary image classifier. The goal is to trick the classifier by changing a specific number of pixels in the image. The classifier, which has 100\% accuracy on a set of training data, accepts an image and attempts to identify it as either X or Y based on the preponderance of the evidence. An optimizer must provide weights to the algorithms before selecting them based on their projected performance in order to attack the classifier. The majority votes of the algorithms determine the compound classifier's output.\\

Five distinct machine learning and/or optimization techniques must be employed independently to attempt to trick the classifier in order to accomplish this project successfully. The algorithms will be placed inside of another algorithm, which must provide them weights based on how well they should perform given the image. The optimizer must then decide which pixels to use out of the set that the algorithms have chosen to alter. A picture and a scalar (that informs the algorithm of the expected budget in terms of the ratio of the image size) will both be inputs to the adversarial function. The group as a whole will pick five sub-algorithms, each one being assigned to each individual group member.\\

The number of successfully deceived photographs at a specific budget level will be used to evaluate the performance of the algorithms. The smaller the budget, the more valuable each successfully fooled image is, and this scales with the size of the image. The project's success criteria are that the model must successfully fool the classifier with exactly B (the pixel budget) pixels changed in the image, and this must be done in less than 10 seconds per image. The model must be able to fool the classifier with a budget of P/100 for at least one image, where P is the number of pixels in the image.\\

To complete this project, a complete justification of the rationale for selecting each of the five machine learning algorithms will be provided. This justification will also cover how they were trained and how the optimizer allocates the votes. Finally, the justification should explain why such an approach should work. All code, notes, documentation, etc. must be tracked using Git and Github for verification of the effort contributed by each team member. 

\section{Problem Being Addressed}
The main problem that our team is aiming to address in this project is to train an algorithm and sub-algorithms that will perform adversarial attacks on a pre-trained image classification model. The pre-trained image classification model is given to us, and must be loaded into our code. This model works by accepting an image, and then proceeds to attempt to classify the image as either X or Y.  It does this classification based on the preponderance of the evidence, that is 51\% probability of an image being an X means the machine will classify it as a X.\\

These sub-algorithms must be placed within one main algorithm. This main algorithm will assign weights to the other five algorithms, along the lines of the weighted majority classifier. The final result must be able to fool the image classifier that is provided with exactly B pixels changed in the image. The main algorithm must assign weights to the different algorithms based on some expected performance given the image. Once the algorithms vote on which set of pixels will be changed, the optimizer will select which pixels to actually use from the set. The only thing that will impact the budget for this project will be the absolute number of pixels used to fool the classifier. The adversarial function input must be both image and scalar, which tells the algorithm the expected budget in terms of the ratio image size. Our model must be able to successfully fool the classifier with a budget of P/100, for at least one image, where P is the number of pixels in the image, and must do this in less than 10 seconds per image.

\section{Solution to the Problem}
In order to accomplish this, the team worked on developing five sub-algorithms to independently attempt to deceive the given classifier. These methods of these five sub-algorithms include the Basic Iterative Method, the dOur team decided that we would independently work on completing the sub-algorithms and then shift towards developing the weighted majority classifier. We believe that by dividing the labor among the group, each team member would be able to specialize in an algorithm and provide higher quality results. Once weights have been assigned, the highest scoring algorithm would be chosen for use against the model. For a successful algorithm, it is expected that it all images in each respective folder, dandelion and grass, would be returned as the model is structured so that if it is unable to categorize the image correctly with a probability greater than fifty percent. 

\newpage
\noindent\section{Selection of Algorithms}
As a group, we have selected five sub-algorithms that we believe will work best for our overall algorithm. These sub-algorithms were carefully selected, and each one was assigned to a group member. Below is a description of each algorithm and how it works. 
\subsection{Fast Gradient Sign Method}
The Fast Gradient Sign Method is a technique used in adversarial machine learning to generate adversarial examples that can fool a machine learning model. The FGSM algorithm takes advantage of the gradient information of the loss function with respect to the input data to generate adversarial examples.\\

To generate an adversarial example using FGSM, the algorithm first computes the gradient of the loss function with respect to the input data. Then, it uses the sign of the gradient to generate a perturbation vector that is added to the original input to create the adversarial example. The magnitude of the perturbation vector is controlled by a hyperparameter epsilon, which determines how far the perturbation can deviate from the original input.\\

The resulting adversarial example is designed to be as close to the original input as possible while still causing the model to make a misclassification. The goal of this attack is to expose the model's weaknesses and vulnerabilities, and to evaluate its robustness and security against potential real-world attacks. The FGSM algorithm is simple and efficient, and can be applied to various types of machine learning models, including neural networks, decision trees, and support vector machines.\\

We chose the FGSM method because it is fast, computationally efficient, and easy to implement, making it a suitable choice for adversarial attacks. It has been shown to be effective against a wide range of deep learning models, including image classifiers. FGSM can successfully fool binary image classifiers with a relatively small number of perturbations, even with a low pixel budget. Additionally, FGSM can be used to generate a large number of adversarial examples quickly, which is important when optimizing the weights of the different sub-algorithms.
\subsection{Basic Iterative Method}
The Basic Iterative Method is a technique that is an extension of the FGSM. Instead of taking large steps, the BIM uses smaller steps and iterations to change the image, which results in higher rate of success. BIM is very straightforward and easy to manipulate using the right libraries in r, we can easily change the input values so that the algorithm run faster for perform better. \\


\subsection{DeepFool} 
The DeepFool algorithm is used to misclassify an image by computing the minimal perturbations required in the most efficient manner. The authors Moosavi-Dezfooli et al. [2] performed their testing alongside the FGSM method in order to compare results and found that although both could misclassify the original image, FGSM required more perturbations and was thus more noticeable than DeepFool. We wanted to include DeepFool so that we could compare it with our own findings for FGSM. In order to do this, the DeepFool algorithm computes the minimal perturbation by finding the smallest distance to teh decision boundary of a model through many iterations. As Morgan [3] writes, "in order to obtain the minimal perturbation for the classifier of 'f' to misclassify the input image of 'x', the perturbation of r*(x) must project the input image of 'x' orthogonal to the hyperplane of the binary classifier.", it applies this approach iteratively to modify the given image until the classified output changes. This is done in small steps so that at least visually, the images look identical. \\

Runtime and performance tradeoffs for DeepFool vary on the size and sophistication of the data but based off DeepFool's authors results, we can get a rough idea of what our results would have looked like. Moosavi-Dezfooli et al. [2] ran DeepFool on six different datasets for one sample, with an average time to compute one sample was 465 ms, applying this to our own data, we were given 37 images of dandelions and 48 images of grass. Each dataset could have taken roughly 17,205 ms for dandelions and 22,320 ms for grass. To contrast, FGSM is able to produce the perturbation much quicker because it only applies one iteration, however the perturbation is on average five times larger than DeepFool's, and as seen in the results section of Moosavi-Dezfooli et al. [2], the accuracy of FGSM was lower across all methods, than DeepFool's due the larger perturbation. \\

To generate testing pairs for DeepFool, we would apply it to a copy of the given data to create a reference point for every sample. We would apply the DeepFool to generate a set of adversarial images that would be used to compare the performance of the model on both the original and perturbed variations. The DeepFool algorithm has no need for knowledge representations because it does not rely on them, instead it proceeds by computing the gradient of the machine learning model and then finding the least distance between the decision boundary and the original point. \\

\subsection{Carlini-Wagner}

The CW attack algorithm is an optimization-based adversarial attack that generates perturbations in the input data to deceive deep learning models. The algorithm focuses on crafting adversarial examples with minimal perturbation to the original input, making it challenging for humans to perceive the difference between the original and the modified input. The CW attack algorithm is highly effective in fooling various deep learning models, including image classifiers, even when the models implement robust defenses.\\

In the given problem, we have a pre-trained image classification model that classifies images as either X or Y, based on the preponderance of evidence. The classifier considers a 51 percent probability of an image being an X as sufficient to classify it as an X. Our goal is to use the CW attack algorithm to fool the classifier into misclassifying the images while adhering to a specific pixel budget. The pixel budget determines the number of pixels that can be altered in the image, which is expressed as a ratio of the total image size (e.g., 0.01 uses 1 percent of the available pixels, 1/P uses only one pixel).\\

The CW attack algorithm operates by solving an optimization problem that seeks to minimize the perturbation added to the original input while maximizing the classifier's misclassification probability. The algorithm comprises three main components, the objective function, binary search, and the gradient descent. The objective function consists of two terms. The first term measures the difference between the original input and the adversarial example, while the second term represents the misclassification probability. The algorithm aims to minimize the objective function by finding the optimal balance between minimizing perturbation and maximizing misclassification. The CW attack employs a binary search strategy to find the optimal balance between the two terms in the objective function. By adjusting a hyperparameter called the "confidence," the algorithm progressively searches for an adversarial example that can successfully fool the classifier. Finally, the algorithm utilizes gradient descent to iteratively update the adversarial example. It calculates the gradient of the objective function with respect to the input and uses it to adjust the adversarial example in the direction that minimizes the objective function.

\subsubsection{Generating Adversarial Image}

The function "generateadversarialexample" generates adversarial images. The function takes three arguments: the input image, the pixelbudgetratio, and the pre-trained model. The goal of this function is to create an adversarial image that fools the classifier while altering a limited number of pixels according to the specified pixel budget ratio.\\

The entire code can be found in Appendix 8.1.1. In the first line the input image is normalized by dividing its pixel value by 255.Normalizing the image ensures that the pixel values fall within the range [0, 1], which is the standard input format for most deep learning models. Next, the pixel budget is calculated by multiplying the total number of pixels in the image (obtained using prod(dim(image))) with the pixelbudgetratio. The ceiling() function rounds up the result to the nearest integer value, ensuring that the pixel budget is a whole number.\\

The Carlini-Wagner attack using the CarliniWagnerL2-function from the 'cleverhans' library. The function takes several arguments, such as the TensorFlow session (sess), the pre-trained model (model), the number of classes (nbclasses), and various hyperparameters controlling the attack's behavior. The maxperturbations parameter is set to the calculated pixelbudget to limit the number of pixels that can be altered in the image. The adverserial image is generated by calling the attack object as generated. 



\subsection{Projected Gradient Descent}

\newpage
\section{Correctness of Proofs}

As a group we chose 5 parameters to base our correctness of proof analysis on. These five are validating the input, creating adversarial examples, bounding the perturbation, evaluating the effectiveness, and examining the convergence properties. \\

\subsection{Fast Gradient Sign}

The Fast Gradient Sign (FGS) algorithm is a popular white-box adversarial attack technique that computes the gradient of the loss function with respect to the input image. The algorithm creates adversarial examples by perturbing the input image with the sign of the gradient multiplied by a small scalar, epsilon. The FGS algorithm is relatively fast and straightforward to implement.\\

The correctness of the FGS algorithm can be analyzed by understanding the assumptions it makes about the model's decision boundaries. FGS assumes that the decision boundaries are linear, which might not be true for complex models like deep neural networks. However, the FGS algorithm has been empirically proven to be effective in fooling image classifiers, even though its theoretical basis relies on simplifying assumptions. This suggests that FGS can be considered as a valid approach for the given problem, but its performance may not be optimal in all cases.\\

The first step in analyzing the correctness of the FGS algorithm is validating the input. The input to the algorithm is an image that needs to be perturbed to fool the classifier. The algorithm assumes that the input image is a valid example from the given distribution and that it is correctly classified by the classifier. The FGS algorithm generates adversarial examples by perturbing the input image with the sign of the gradient of the loss function with respect to the input image, multiplied by a small scalar, epsilon. The gradient information is obtained by computing the gradient of the loss function for the given input image and target class. The sign of the gradient is used to determine the direction in which the pixel values should be perturbed to maximize the loss function.The algorithm perturbs the input image by adding the scaled sign of the gradient, which can result in pixel values that lie outside the valid range. To ensure that the adversarial examples remain valid and perceptually similar to the original image, the perturbations must be constrained within a specific range. The final aspect of the correctness of proof analysis for the FGS algorithm is examining its convergence properties. The convergence properties of the algorithm describe the stability and speed at which the algorithm converges to an adversarial example. Ideally, the algorithm should converge quickly to generate effective adversarial examples with minimal computation.

\subsection{Basic Iterative}

The Basic Iterative (BIM) algorithm is an extension of the FGS algorithm that applies the FGS method iteratively, allowing for more precise perturbations. The algorithm performs a fixed number of iterations, applying the FGS method at each step and clipping the resulting adversarial image to ensure that the pixel values remain within a valid range.\\

The correctness of the BI algorithm relies on the same assumptions as the FGS algorithm, but its iterative nature allows for a more fine-grained search of the adversarial perturbations. The BI algorithm has been demonstrated to produce more effective adversarial examples than the FGS algorithm, while still maintaining a relatively low computational cost. Therefore, the BI algorithm can be considered as a valid approach for the given problem, with the caveat that its performance may also be affected by the simplifying assumptions regarding the decision boundaries.\\

\subsection{Deep Fool}

The Deep Fool (DF) algorithm is a more advanced adversarial attack technique that aims to find the minimal perturbation required to cross the decision boundary of the classifier. The algorithm iteratively linearizes the decision boundary around the current input image and computes the minimal perturbation to cross the linearized boundary. This process is repeated until the adversarial example is misclassified by the model.\\

The correctness of the DF algorithm relies on the assumption that the decision boundaries are locally linear, which is a more relaxed assumption compared to the FGS and BI algorithms. The DF algorithm has been shown to produce more effective and minimal adversarial perturbations, but it has a higher computational cost due to the iterative nature of the algorithm. In the context of the given problem, the DF algorithm can be considered as a valid approach, as it offers a more accurate estimation of the decision boundaries, albeit at a higher computational cost.\\

\subsection{Carlini Wagner}

The Carlini Wagner (CW) algorithm is a more sophisticated attack method that optimizes an objective function that combines the model's loss and a distance metric between the original and adversarial images. The optimization is performed using a gradient-based approach, which allows for a more targeted search of the adversarial perturbations.\\

The correctness of the CW algorithm relies on its ability to accurately model the relationship between the input image and the model's output. The CW algorithm has been demonstrated to produce highly effective adversarial examples, even against models with defensive mechanisms. However, the CW algorithm has a higher computational cost due to the optimization process,which might be a limiting factor in the context of the given problem. Despite the increased computational cost, the CW algorithm can be considered as a valid approach, as it provides a more principled way to generate adversarial perturbations and has proven effective in fooling a wide range of classifiers.\\

\subsection{Projected Gradient Descent}

The Projected Gradient Descent (PGD) algorithm is another advanced adversarial attack method that extends the FGS algorithm. The PGD algorithm performs a gradient-based optimization to find adversarial perturbations, but it projects the resulting perturbations onto a predefined constraint set to ensure that the pixel values remain within a valid range. This projection step makes the PGD algorithm more robust against various defense mechanisms.\\

The correctness of the PGD algorithm relies on its ability to accurately model the gradient information of the classifier while adhering to the constraint set. The PGD algorithm has been shown to produce effective adversarial examples and has been used as a benchmark for evaluating the robustness of different classifiers. The PGD algorithm has a higher computational cost compared to the FGS and BI algorithms, but its projection step makes it more robust against defensive mechanisms. In the context of the given problem, the PGD algorithm can be considered as a valid approach, as it offers a more robust way to generate adversarial perturbations while maintaining the pixel budget constraints.\\

\subsection{Main Algorithm}

The main algorithm creates adversarial examples by combining the outputs of the five sub-algorithms using a weighted majority classifier. This ensemble approach leverages the strengths of each sub-algorithm while mitigating their individual weaknesses. The correctness of the main algorithm in creating adversarial examples relies on its ability to assign appropriate weights to the different sub-algorithms based on their expected performance given the input image.\\

The main algorithm's success in generating adversarial examples is determined by how effectively it combines the outputs of the sub-algorithms. By assigning appropriate weights to each sub-algorithm and aggregating their results, the main algorithm can generate more effective adversarial examples that can fool the image classifier.\\

The main algorithm's perturbation bounding is determined by the individual sub-algorithms' perturbation constraints. The main algorithm can maintain the pixel budget constraints by appropriately assigning weights to the sub-algorithms based on their ability to generate adversarial examples within the given budget. As long as the sub-algorithms adhere to the perturbation constraints, the main algorithm will generate adversarial examples that respect the pixel budget constraints. The effectiveness of the main algorithm can be evaluated by measuring the success rate of the generated adversarial examples in fooling the classifier. A high success rate indicates that the main algorithm effectively combines the outputs of the sub-algorithms and generates adversarial examples that can mislead the classifier. The correctness of the main algorithm's proof relies on its ability to generate effective adversarial examples by leveraging the strengths of the individual sub-algorithms and mitigating their weaknesses.\\

\newpage
\section{Complexity Analysis}

\clearpage
\section{Appendix}

\subsection{Carlini-Wagner function}
\subsubsection{Adverserial Image Generation}

Below is the function that generates adverserial images using the Carlini Wagner method

\begin{lstlisting}[language=R, frame=single]
generateadversarialexample <- function(image, pixelbudgetratio, model) {
  # Normalize the input image
  image <- image / 255
  
  # Calculate the pixel budget
  pixelbudget <- ceiling(prod(dim(image)) * pixelbudgetratio)
  
  # Initialize the Carlini-Wagner attack
  attack <- CarliniWagnerL2(sess, model, nb_classes = 2, batch_size = 1,
                             confidence = 0, targeted = FALSE, learning_rate = 0.01,
                             binary_search_steps = 9, max_iterations = 1000,
                             abort_early = TRUE, initial_const = 0.01,
                             clip_min = 0, clip_max = 1,
                             maxperturbations = pixelbudget)
  
  # Generate the adversarial example
  adversarial_image <- attack$attack(image)
  
  return(adversarial_image)
}

# generate adverserial image

# Load an image from your dataset
image <- ... # Load your specific image here

# Set the pixel budget ratio
pixel_budget_ratio <- 0.01  # Adjust this value as needed

# Generate the adversarial example
adversarial_image <- generate_adversarial_example(image, pixel_budget_ratio, model)
\end{lstlisting}

\subsubsection{Testing}

\begin{lstlisting}[language=R, frame=single]

# Display the original and adversarial images
par(mfrow = c(1, 2))
plot(as.raster(image), main = "Original Image")
plot(as.raster(adversarial_image), main = "Adversarial Image")

# Test the classifier
original_pred <- which.max(predict(model, array_reshape(image, c(1, dim(image)))))
adversarial_pred <- which.max(predict(model, array_reshape(adversarial_image, c(1, dim(adversarial_image)))))

cat("Original Image Prediction:", original_pred, "\n")
cat("Adversarial Image Prediction:", adversarial_pred, "\n")

\end{lstlisting}

\subsection{Testing/Correctness/Verification}

\subsubsection{Testing of the Carlini-Wagner algorithim}

 The goal of the script is to see if the generated adverserial image actually fooled the classifier. The entire code can be found in the Appendix section 8.1.2\\

The par(mfrow = c(1, 2)) function call sets up a plotting area with one row and two columns, allowing us to display two images side by side. The two plot functions display the original and adversarial images, respectively, with their corresponding titles. This visual comparison helps us confirm that the adversarial image is very similar to the original image, with only subtle differences introduced by the Carlini-Wagner attack algorithm.\\

To test the classifier, the predict() function is used to obtain the model's output probabilities for each class (X or Y) when presented with the original and adversarial images. The array reshape() function reshapes the images into a format suitable for the classifier. The which.max() function returns the index of the maximum probability, which corresponds to the predicted class.\\

Finally, the cat() function to display the predicted classes for the original and adversarial images. If the Carlini-Wagner attack algorithm is successful, the classifier's prediction for the adversarial image should differ from the original image's prediction, despite the visual similarity between the two images. This outcome demonstrates the algorithm's effectiveness in fooling the image classifier while adhering to the specified pixel budget.\\

\newpage\subsection{Runtime Complexity and Walltime}

\newpage\subsection{Performance}

\newpage\subsection{Final Algorithm Selection Justification}

The final algorithm will run through all 5 sub-algorithms (Fast Gradient Sign, Basic Iterative, Deep Fool, Carlini-Wagner, and Projected Gradient) at 10 iterations. For each algorithm a "success index" is made based on the number of successfully fooled images at 10 iterations per image. The algorithm with the highest success index is the one that will be chosen to run through all the iterations.  

\newpage
\section{References}
\begin{enumerate}
    \item ewatson2. (n.d.). Ewatson2/EEL6812\textunderscore DeepFool\textunderscore Project: Class project implementing the deepfool adversarial attack. GitHub. Retrieved April 28, 
    2023, from https://github.com/ewatson2/EEL6812\textunderscore DeepFool\textunderscore Project 

    \item Moosavi-Dezfooli, S.-M., Fawzi, A., &amp; Frossard, P. (2016, July 4). DeepFool: A simple and accurate method to fool Deep Neural Networks. arXiv.org. Retrieved April 28, 2023, from https://arxiv.org/abs/1511.04599
    
    \item Morgan, A. (2022, May 2). A review of DeepFool: A simple and accurate method to fool Deep Neural Networks. Medium. Retrieved April 28, 2023, from https://medium.com/machine-intelligence-and-deep-learning-lab/a-review-of-deepfool-a-simple-and-accurate-method-to-fool-deep-neural-networks-b016fba9e48e 
    
    \item 
    
\end{enumerate}

\end{document}
